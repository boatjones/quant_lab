{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0e341c22",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "import logging\n",
    "from datetime import date\n",
    "\n",
    "# Simulate __file__ in Jupyter\n",
    "__file__ = str(Path.cwd() / \"tiingo_maintainance_test.ipynb\")\n",
    "\n",
    "# Get the path to your secrets directory\n",
    "project_root = Path(__file__).parents[1]\n",
    "sys.path.insert(0, str(project_root))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7ed65c3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# must come after path is set\n",
    "from util.to_postgres import PgHook\n",
    "from util.tiingo_manager import TiingoDataManager"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "83597332",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/rthomas/thomasr/quant_lab\n"
     ]
    }
   ],
   "source": [
    "print(sys.path[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f7d09867",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up logging\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format='%(asctime)s - %(levelname)s - %(message)s',\n",
    "    handlers=[\n",
    "        logging.FileHandler('tiingo_load.log'),\n",
    "        logging.StreamHandler()\n",
    "    ]\n",
    ")\n",
    "logger = logging.getLogger(__name__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0f9134b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# instantiate Postgres database utility\n",
    "db = PgHook()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1c89f5d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-12-02 13:47:57,753 - INFO - Initialize TiingoDataManager with rate limiting\n"
     ]
    }
   ],
   "source": [
    "# Instantiate Tiingo class\n",
    "tdm = TiingoDataManager()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e09fbb74",
   "metadata": {},
   "source": [
    "#### Update Symbols Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2359bcc2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-12-02 13:48:01,949 - INFO - Fetching all tickers from Tiingo...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fetching ticker universe...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-12-02 13:48:02,437 - INFO - Filtered out 4746 junk tickers (warrants, units, preferreds)\n"
     ]
    }
   ],
   "source": [
    "# Get all tickers from Tiingo\n",
    "print(\"Fetching ticker universe...\")\n",
    "tiingo_df = tdm.get_all_tickers(include_delisted=False, filter_junk=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9bbae171",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get current tickers and active status\n",
    "cur_symbols_df = db.psy_query(\"select ticker, is_active as is_active_old from symbols\")\n",
    "\n",
    "# Merge in current active flag to new records\n",
    "existing_df = tiingo_df.merge(\n",
    "    cur_symbols_df[['ticker', 'is_active_old']],\n",
    "    on='ticker',\n",
    "    how='inner'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2ec934e4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ticker</th>\n",
       "      <th>is_active_old</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>A</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>AA</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>AAA</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>GSPC</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>DJI</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  ticker  is_active_old\n",
       "0      A              1\n",
       "1     AA              1\n",
       "2    AAA              1\n",
       "3   GSPC              1\n",
       "4    DJI              1"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# DEBUG\n",
    "cur_symbols_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d24c906d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "12999"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# DEBUG\n",
    "len(cur_symbols_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53ad6a6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# DEBUG: temporarily stash original symbols tickers\n",
    "db.alc_df_2_db_r(cur_symbols_df, 'temp_orig_symbols')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "336cb81e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# DEBUG: save tiingo download\n",
    "db.alc_df_2_db_r(tiingo_df, 'temp_tiingo')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e58533b7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ticker</th>\n",
       "      <th>company_name</th>\n",
       "      <th>exchange</th>\n",
       "      <th>asset_type</th>\n",
       "      <th>start_date</th>\n",
       "      <th>end_date</th>\n",
       "      <th>is_active</th>\n",
       "      <th>is_etf</th>\n",
       "      <th>is_active_old</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>A</td>\n",
       "      <td>None</td>\n",
       "      <td>NYSE</td>\n",
       "      <td>stock</td>\n",
       "      <td>1999-11-18</td>\n",
       "      <td>None</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>AA</td>\n",
       "      <td>None</td>\n",
       "      <td>NYSE</td>\n",
       "      <td>stock</td>\n",
       "      <td>2016-10-18</td>\n",
       "      <td>None</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>AAA</td>\n",
       "      <td>None</td>\n",
       "      <td>NYSE ARCA</td>\n",
       "      <td>etf</td>\n",
       "      <td>2020-09-09</td>\n",
       "      <td>None</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>AAAU</td>\n",
       "      <td>None</td>\n",
       "      <td>BATS</td>\n",
       "      <td>etf</td>\n",
       "      <td>2018-08-15</td>\n",
       "      <td>None</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>AAC</td>\n",
       "      <td>None</td>\n",
       "      <td>NYSE</td>\n",
       "      <td>stock</td>\n",
       "      <td>2021-03-25</td>\n",
       "      <td>None</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  ticker company_name   exchange asset_type  start_date end_date  is_active  \\\n",
       "0      A         None       NYSE      stock  1999-11-18     None          1   \n",
       "1     AA         None       NYSE      stock  2016-10-18     None          1   \n",
       "2    AAA         None  NYSE ARCA        etf  2020-09-09     None          1   \n",
       "3   AAAU         None       BATS        etf  2018-08-15     None          1   \n",
       "4    AAC         None       NYSE      stock  2021-03-25     None          1   \n",
       "\n",
       "   is_etf  is_active_old  \n",
       "0       0              1  \n",
       "1       0              1  \n",
       "2       1              1  \n",
       "3       1              1  \n",
       "4       0              0  "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# DEBUG\n",
    "existing_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "fc7a734b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Update newly inactive tickers and put in end date  \n",
    "newly_inactive_mask = (existing_df['is_active'] == 0) & (existing_df['is_active_old'] == 1)\n",
    "existing_df.loc[newly_inactive_mask, 'end_date'] = date.today()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1157f773",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# DEBUG: Newly inactive tickers\n",
    "newly_inactive_mask.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "276e8536",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    False\n",
       "1    False\n",
       "2    False\n",
       "3    False\n",
       "4    False\n",
       "dtype: bool"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# DEBUG\n",
    "newly_inactive_mask.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "6e2b5679",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "12967"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# DEBUG\n",
    "len(existing_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11d11257",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ticker</th>\n",
       "      <th>company_name</th>\n",
       "      <th>exchange</th>\n",
       "      <th>asset_type</th>\n",
       "      <th>start_date</th>\n",
       "      <th>end_date</th>\n",
       "      <th>is_active</th>\n",
       "      <th>is_etf</th>\n",
       "      <th>is_active_old</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>A</td>\n",
       "      <td>None</td>\n",
       "      <td>NYSE</td>\n",
       "      <td>stock</td>\n",
       "      <td>1999-11-18</td>\n",
       "      <td>None</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>AA</td>\n",
       "      <td>None</td>\n",
       "      <td>NYSE</td>\n",
       "      <td>stock</td>\n",
       "      <td>2016-10-18</td>\n",
       "      <td>None</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>AAA</td>\n",
       "      <td>None</td>\n",
       "      <td>NYSE ARCA</td>\n",
       "      <td>etf</td>\n",
       "      <td>2020-09-09</td>\n",
       "      <td>None</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>AAAU</td>\n",
       "      <td>None</td>\n",
       "      <td>BATS</td>\n",
       "      <td>etf</td>\n",
       "      <td>2018-08-15</td>\n",
       "      <td>None</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>AAC</td>\n",
       "      <td>None</td>\n",
       "      <td>NYSE</td>\n",
       "      <td>stock</td>\n",
       "      <td>2014-10-02</td>\n",
       "      <td>None</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  ticker company_name   exchange asset_type  start_date end_date  is_active  \\\n",
       "0      A         None       NYSE      stock  1999-11-18     None          1   \n",
       "1     AA         None       NYSE      stock  2016-10-18     None          1   \n",
       "2    AAA         None  NYSE ARCA        etf  2020-09-09     None          1   \n",
       "3   AAAU         None       BATS        etf  2018-08-15     None          1   \n",
       "4    AAC         None       NYSE      stock  2014-10-02     None          1   \n",
       "\n",
       "   is_etf  is_active_old  \n",
       "0       0              1  \n",
       "1       0              1  \n",
       "2       1              1  \n",
       "3       1              1  \n",
       "4       0              1  "
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# DEBUG\n",
    "existing_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d62873b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Newly inactive tickers to update: 0\n"
     ]
    }
   ],
   "source": [
    "# Get dataframe of newly inactive tickers\n",
    "updates_df = existing_df.loc[\n",
    "    existing_df['end_date'].notna(),\n",
    "    ['ticker', 'is_active', 'end_date']\n",
    "]\n",
    "print(f\"Newly inactive tickers to update: {len(updates_df)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "158a19f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-12-02 13:48:35,817 - INFO - No updates on 'is_active' to symbols.\n"
     ]
    }
   ],
   "source": [
    "if len(updates_df) > 0:\n",
    "    # update symbols table in database\n",
    "    for _, row in updates_df.iterrows():\n",
    "        sql = \"\"\"\n",
    "            UPDATE symbols \n",
    "            SET is_active = %s, end_date = %s\n",
    "            WHERE ticker = %s\n",
    "        \"\"\"\n",
    "        db.execute_sql(sql, (row['is_active'], row['end_date'], row['ticker']))\n",
    "\n",
    "    logger.info(f\"Updated {len(updates_df)} symbols with inactive status\")\n",
    "\n",
    "else:\n",
    "    logger.info(\"No updates on 'is_active' to symbols.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "fa1dc2ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New tickers to add: 7\n"
     ]
    }
   ],
   "source": [
    "# Find new tickers (exclude known junk)\n",
    "excluded_df = db.psy_query(\"select ticker from excluded_tickers\")\n",
    "new_tickers_df = tiingo_df[\n",
    "    ~tiingo_df['ticker'].isin(cur_symbols_df['ticker']) &\n",
    "    ~tiingo_df['ticker'].isin(excluded_df['ticker'])\n",
    "]\n",
    "\n",
    "print(f\"New tickers to add: {len(new_tickers_df)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "004e3842",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-12-02 13:48:47,420 - INFO - Enriched 1 company names from all_symbols table\n",
      "2025-12-02 13:48:47,420 - INFO - Remaining nulls: 6\n"
     ]
    }
   ],
   "source": [
    "# Check all_symbols table FIRST (fast - no API calls)\n",
    "all_symbols_df = db.psy_query('select ticker, company_name from all_symbols')\n",
    "new_tickers_df = tdm.merge_names(new_tickers_df, all_symbols_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f5834ef4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tickers needing API enrichment: 6\n"
     ]
    }
   ],
   "source": [
    "# Only enrich tickers that STILL have no name\n",
    "needs_enrichment = new_tickers_df[new_tickers_df['company_name'].isna()]['ticker'].tolist()\n",
    "print(f\"Tickers needing API enrichment: {len(needs_enrichment)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c70c3f55",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-12-02 13:49:06,866 - INFO - Null company names: 0, Empty company names: 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Completed 7 iterations, 0 errors\n"
     ]
    }
   ],
   "source": [
    "# Get company names for new tickers\n",
    "enriched_ticker_df = tdm.enrich_company_names(new_tickers_df['ticker'].to_list())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "afa02ca9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  ticker                                       company_name\n",
      "0   DDFD  Innovator Equity Dual Directional 15 Buffer ET...\n",
      "1   DDTD  Innovator Equity Dual Directional 10 Buffer ET...\n",
      "2   FMTO                             Femto Technologies Inc\n",
      "3   GRDX                            Entero Therapeutics Inc\n",
      "4   PTPI                         Petros Pharmaceuticals Inc\n",
      "5    SMQ          Tradr 1X Short Innovation 100 Monthly ETF\n",
      "6   XAGE                      Longevity Health Holdings Inc\n"
     ]
    }
   ],
   "source": [
    "# DEBUG\n",
    "print(enriched_ticker_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "d5b7f615",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-12-02 13:49:30,228 - INFO - Enriched 6 company names from all_symbols table\n",
      "2025-12-02 13:49:30,229 - INFO - Remaining nulls: 0\n"
     ]
    }
   ],
   "source": [
    "# Merge company names into tiingo_df to make new_symbols_df\n",
    "new_tickers_df = tdm.merge_names(new_tickers_df, enriched_ticker_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "1cefceec",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-12-02 13:49:35,253 - INFO - Corrected 3 ETFs mislabeled as stocks\n"
     ]
    }
   ],
   "source": [
    "# After getting new_tickers_df, before filtering\n",
    "# Fix mislabeled ETFs - check company name only\n",
    "etf_mask = (\n",
    "    (new_tickers_df['asset_type'] == 'stock') &\n",
    "    new_tickers_df['company_name'].notna() &\n",
    "    (\n",
    "        new_tickers_df['company_name'].str.contains(' ETF ', case=False, na=False) |\n",
    "        new_tickers_df['company_name'].str.lower().str.endswith('etf')\n",
    "    )\n",
    ")\n",
    "\n",
    "new_tickers_df.loc[etf_mask, 'asset_type'] = 'etf'\n",
    "new_tickers_df.loc[etf_mask, 'is_etf'] = 1\n",
    "\n",
    "logger.info(f\"Corrected {etf_mask.sum()} ETFs mislabeled as stocks\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "65a3ca24",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ticker</th>\n",
       "      <th>company_name</th>\n",
       "      <th>exchange</th>\n",
       "      <th>asset_type</th>\n",
       "      <th>start_date</th>\n",
       "      <th>end_date</th>\n",
       "      <th>is_active</th>\n",
       "      <th>is_etf</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>6546</th>\n",
       "      <td>DDFD</td>\n",
       "      <td>Innovator Equity Dual Directional 15 Buffer ET...</td>\n",
       "      <td>BATS</td>\n",
       "      <td>etf</td>\n",
       "      <td>2025-12-01</td>\n",
       "      <td>None</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6574</th>\n",
       "      <td>DDTD</td>\n",
       "      <td>Innovator Equity Dual Directional 10 Buffer ET...</td>\n",
       "      <td>BATS</td>\n",
       "      <td>etf</td>\n",
       "      <td>2025-12-01</td>\n",
       "      <td>None</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9493</th>\n",
       "      <td>FMTO</td>\n",
       "      <td>Femto Technologies Inc</td>\n",
       "      <td>NASDAQ</td>\n",
       "      <td>stock</td>\n",
       "      <td>2022-05-31</td>\n",
       "      <td>None</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11016</th>\n",
       "      <td>GRDX</td>\n",
       "      <td>Entero Therapeutics Inc</td>\n",
       "      <td>NASDAQ</td>\n",
       "      <td>stock</td>\n",
       "      <td>2016-10-11</td>\n",
       "      <td>None</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20471</th>\n",
       "      <td>PTPI</td>\n",
       "      <td>Petros Pharmaceuticals, Inc.</td>\n",
       "      <td>NASDAQ</td>\n",
       "      <td>stock</td>\n",
       "      <td>2020-12-02</td>\n",
       "      <td>None</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23119</th>\n",
       "      <td>SMQ</td>\n",
       "      <td>Tradr 1X Short Innovation 100 Monthly ETF</td>\n",
       "      <td>BATS</td>\n",
       "      <td>etf</td>\n",
       "      <td>2025-12-01</td>\n",
       "      <td>None</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27515</th>\n",
       "      <td>XAGE</td>\n",
       "      <td>Longevity Health Holdings Inc</td>\n",
       "      <td>NASDAQ</td>\n",
       "      <td>stock</td>\n",
       "      <td>2021-09-23</td>\n",
       "      <td>None</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      ticker                                       company_name exchange  \\\n",
       "6546    DDFD  Innovator Equity Dual Directional 15 Buffer ET...     BATS   \n",
       "6574    DDTD  Innovator Equity Dual Directional 10 Buffer ET...     BATS   \n",
       "9493    FMTO                             Femto Technologies Inc   NASDAQ   \n",
       "11016   GRDX                            Entero Therapeutics Inc   NASDAQ   \n",
       "20471   PTPI                       Petros Pharmaceuticals, Inc.   NASDAQ   \n",
       "23119    SMQ          Tradr 1X Short Innovation 100 Monthly ETF     BATS   \n",
       "27515   XAGE                      Longevity Health Holdings Inc   NASDAQ   \n",
       "\n",
       "      asset_type  start_date end_date  is_active  is_etf  \n",
       "6546         etf  2025-12-01     None          1       1  \n",
       "6574         etf  2025-12-01     None          1       1  \n",
       "9493       stock  2022-05-31     None          1       0  \n",
       "11016      stock  2016-10-11     None          1       0  \n",
       "20471      stock  2020-12-02     None          1       0  \n",
       "23119        etf  2025-12-01     None          1       1  \n",
       "27515      stock  2021-09-23     None          1       0  "
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# DEBUG\n",
    "new_tickers_df.head(30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6975dab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FT Vest U.S. Equity Dual Directional Buffer ETF November\n",
      "Length: 56\n"
     ]
    }
   ],
   "source": [
    "# DEBUG: Get the actual full value\n",
    "full_name = new_tickers_df[new_tickers_df['ticker'] == 'DLNV']['company_name'].iloc[0]\n",
    "print(full_name)\n",
    "print(f\"Length: {len(full_name)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "0d908d45",
   "metadata": {},
   "outputs": [],
   "source": [
    "# After all enrichment attempts...\n",
    "new_symbols_df = new_tickers_df[\n",
    "    new_tickers_df['company_name'].notna() & \n",
    "    (new_tickers_df['company_name'] != '') &\n",
    "    (new_tickers_df['company_name'].str.lower() != 'null')\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "d0b62178",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save junk tickers so we skip them next time\n",
    "junk_tickers = new_tickers_df[\n",
    "    new_tickers_df['company_name'].isna() | \n",
    "    (new_tickers_df['company_name'] == '') |\n",
    "    (new_tickers_df['company_name'].str.lower() == 'null')\n",
    "]['ticker'].tolist()\n",
    "\n",
    "if junk_tickers:\n",
    "    for ticker in junk_tickers:\n",
    "        sql = \"\"\"\n",
    "            INSERT INTO excluded_tickers (ticker, reason)\n",
    "            VALUES (%s, %s)\n",
    "            ON CONFLICT (ticker) DO NOTHING\n",
    "        \"\"\"\n",
    "        db.execute_sql(sql, (ticker, 'no_company_name'))\n",
    "    logger.info(f\"Added {len(junk_tickers)} to excluded_tickers\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "44f58593",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ticker</th>\n",
       "      <th>company_name</th>\n",
       "      <th>exchange</th>\n",
       "      <th>asset_type</th>\n",
       "      <th>start_date</th>\n",
       "      <th>end_date</th>\n",
       "      <th>is_active</th>\n",
       "      <th>is_etf</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>6546</th>\n",
       "      <td>DDFD</td>\n",
       "      <td>Innovator Equity Dual Directional 15 Buffer ET...</td>\n",
       "      <td>BATS</td>\n",
       "      <td>etf</td>\n",
       "      <td>2025-12-01</td>\n",
       "      <td>None</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6574</th>\n",
       "      <td>DDTD</td>\n",
       "      <td>Innovator Equity Dual Directional 10 Buffer ET...</td>\n",
       "      <td>BATS</td>\n",
       "      <td>etf</td>\n",
       "      <td>2025-12-01</td>\n",
       "      <td>None</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9493</th>\n",
       "      <td>FMTO</td>\n",
       "      <td>Femto Technologies Inc</td>\n",
       "      <td>NASDAQ</td>\n",
       "      <td>stock</td>\n",
       "      <td>2022-05-31</td>\n",
       "      <td>None</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11016</th>\n",
       "      <td>GRDX</td>\n",
       "      <td>Entero Therapeutics Inc</td>\n",
       "      <td>NASDAQ</td>\n",
       "      <td>stock</td>\n",
       "      <td>2016-10-11</td>\n",
       "      <td>None</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20471</th>\n",
       "      <td>PTPI</td>\n",
       "      <td>Petros Pharmaceuticals, Inc.</td>\n",
       "      <td>NASDAQ</td>\n",
       "      <td>stock</td>\n",
       "      <td>2020-12-02</td>\n",
       "      <td>None</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23119</th>\n",
       "      <td>SMQ</td>\n",
       "      <td>Tradr 1X Short Innovation 100 Monthly ETF</td>\n",
       "      <td>BATS</td>\n",
       "      <td>etf</td>\n",
       "      <td>2025-12-01</td>\n",
       "      <td>None</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27515</th>\n",
       "      <td>XAGE</td>\n",
       "      <td>Longevity Health Holdings Inc</td>\n",
       "      <td>NASDAQ</td>\n",
       "      <td>stock</td>\n",
       "      <td>2021-09-23</td>\n",
       "      <td>None</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      ticker                                       company_name exchange  \\\n",
       "6546    DDFD  Innovator Equity Dual Directional 15 Buffer ET...     BATS   \n",
       "6574    DDTD  Innovator Equity Dual Directional 10 Buffer ET...     BATS   \n",
       "9493    FMTO                             Femto Technologies Inc   NASDAQ   \n",
       "11016   GRDX                            Entero Therapeutics Inc   NASDAQ   \n",
       "20471   PTPI                       Petros Pharmaceuticals, Inc.   NASDAQ   \n",
       "23119    SMQ          Tradr 1X Short Innovation 100 Monthly ETF     BATS   \n",
       "27515   XAGE                      Longevity Health Holdings Inc   NASDAQ   \n",
       "\n",
       "      asset_type  start_date end_date  is_active  is_etf  \n",
       "6546         etf  2025-12-01     None          1       1  \n",
       "6574         etf  2025-12-01     None          1       1  \n",
       "9493       stock  2022-05-31     None          1       0  \n",
       "11016      stock  2016-10-11     None          1       0  \n",
       "20471      stock  2020-12-02     None          1       0  \n",
       "23119        etf  2025-12-01     None          1       1  \n",
       "27515      stock  2021-09-23     None          1       0  "
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_symbols_df.head(30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "7960d0ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-12-02 13:50:24,055 - INFO - Upserting 7 symbols...\n",
      "2025-12-02 13:50:24,136 - INFO - Processed 7/7 symbols\n",
      "2025-12-02 13:50:24,137 - INFO - Successfully upserted 7 symbols, 0 failed\n",
      "2025-12-02 13:50:24,137 - INFO - Failed symbols inserted: 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving symbols...\n"
     ]
    }
   ],
   "source": [
    "# Insert new symbol records to table - this needs to be changed to only upsert NEW tickers\n",
    "print(\"Saving symbols...\")\n",
    "failed_symbols = tdm.upsert_symbols(new_symbols_df)\n",
    "logger.info(f\"Failed symbols inserted: {len(failed_symbols)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26ee7ae5",
   "metadata": {},
   "source": [
    "#### Update Stocks Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "2db057d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get current stock records\n",
    "cur_stock_df = db.psy_query(\"select * from stocks\")\n",
    "\n",
    "# Get stocks out of refreshed symbols table\n",
    "new_stock_df = db.psy_query(\"select ticker, company_name, exchange from symbols where asset_type = 'stock'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "e574f971",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find new stock records that are not in current stock records\n",
    "addl_stocks_df = new_stock_df[~new_stock_df['ticker'].isin(cur_stock_df['ticker'])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "a4a96484",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# DEBUG\n",
    "len(addl_stocks_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "01ab4155",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-12-02 13:51:05,880 - INFO - Fetching metadata from yFinance...\n",
      "2025-12-02 13:51:06,280 - INFO - Found 1 tickers from yFinance.\n",
      "2025-12-02 13:51:06,284 - INFO - Upserting 1 stocks...\n",
      "2025-12-02 13:51:06,305 - INFO - Processed 1/1 symbols\n",
      "2025-12-02 13:51:06,306 - INFO - Successfully upserted 1 symbols, 0 failed\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "yFinance returned 1 rows\n",
      "yFinance columns: ['ticker', 'company_name_yf', 'is_etf_yf', 'industry', 'sector', 'exchange_yf']\n"
     ]
    }
   ],
   "source": [
    "if len(addl_stocks_df) == 0:\n",
    "    logger.info(\"No new stocks to add\")\n",
    "else:\n",
    "    # Get industry & sector from yFinance for new stock records\n",
    "    yf_enriched_df = tdm.yfinance_metadata(addl_stocks_df['ticker'].tolist())\n",
    "\n",
    "    # DEBUG: See what's returned from yFinance\n",
    "    print(f\"yFinance returned {len(yf_enriched_df)} rows\")\n",
    "    print(f\"yFinance columns: {yf_enriched_df.columns.tolist()}\")\n",
    "\n",
    "    if len(yf_enriched_df) > 0 and 'industry' in yf_enriched_df.columns:\n",
    "        # merge yFinance data into addl_stocks_df\n",
    "        addl_stocks_df = addl_stocks_df.merge(\n",
    "            yf_enriched_df[['ticker', 'industry', 'sector']],\n",
    "            on='ticker',\n",
    "            how='left'\n",
    "        )\n",
    "    else:\n",
    "        logger.warning(\"yFinance returns no usable data\")\n",
    "        addl_stocks_df['industry'] = None\n",
    "        addl_stocks_df['sector'] = None\n",
    "\n",
    "    # Get industry & sector from FMP for any remaining null or empty records\n",
    "    missing_mask = addl_stocks_df['industry'].isna() | (addl_stocks_df['industry'] == '')\n",
    "    tickers_needing_fmp = addl_stocks_df.loc[missing_mask, 'ticker'].tolist()\n",
    "\n",
    "    if tickers_needing_fmp:\n",
    "        logger.info(f\"Fetching FMP data for {len(tickers_needing_fmp)} tickers missing industry & sector\")\n",
    "        fmp_enriched_df = tdm.fetch_industry_sector(tickers_needing_fmp)\n",
    "\n",
    "        # DEBUG: See what's returned from FMP\n",
    "        print(f\"FMP returned {len(fmp_enriched_df)} rows\")\n",
    "        print(f\"FMP columns: {fmp_enriched_df.columns.tolist()}\")\n",
    "\n",
    "        # Only merge if we got data back\n",
    "        if len(fmp_enriched_df) > 0:\n",
    "            addl_stocks_df = addl_stocks_df.merge(\n",
    "                fmp_enriched_df[['ticker', 'industry', 'sector']],\n",
    "                on='ticker',\n",
    "                how='left',\n",
    "                suffixes=('', '_fmp')\n",
    "            )\n",
    "\n",
    "            addl_stocks_df['industry'] = addl_stocks_df['industry'].fillna(addl_stocks_df['industry_fmp'])\n",
    "            addl_stocks_df['sector'] = addl_stocks_df['sector'].fillna(addl_stocks_df['sector_fmp'])\n",
    "\n",
    "            addl_stocks_df = addl_stocks_df.drop(columns=['industry_fmp', 'sector_fmp'], errors='ignore')\n",
    "\n",
    "        else:\n",
    "            logger.warning(\"FMP returned no data for any tickers\")\n",
    "\n",
    "    # Set field order\n",
    "    stocks_df = addl_stocks_df[['ticker', 'company_name', 'industry', 'sector', 'exchange']]\n",
    "\n",
    "    # Save junk stocks missing industry data so we skip them next time\n",
    "    junk_stocks = stocks_df[\n",
    "        stocks_df['industry'].isna() | \n",
    "        (stocks_df['industry'] == '')\n",
    "    ]['ticker'].tolist()\n",
    "\n",
    "    if junk_stocks:\n",
    "        for ticker in junk_stocks:\n",
    "            sql = \"\"\"\n",
    "                INSERT INTO excluded_tickers (ticker, reason)\n",
    "                VALUES (%s, %s)\n",
    "                ON CONFLICT (ticker) DO NOTHING\n",
    "            \"\"\"\n",
    "            db.execute_sql(sql, (ticker, 'no_industry_data'))\n",
    "\n",
    "        # Delete from symbols table as well\n",
    "        delete_sql = \"\"\"\n",
    "            DELETE FROM symbols\n",
    "            WHERE ticker in (SELECT ticker from excluded_tickers)\n",
    "        \"\"\"\n",
    "        db.execute_sql(delete_sql)\n",
    "        logger.info(f\"Added {len(junk_stocks)} to excluded_tickers and removed from symbols\")\n",
    "\n",
    "    # After all enrichment attempts...\n",
    "    clean_stocks_df = stocks_df[\n",
    "        stocks_df['industry'].notna() & \n",
    "        (stocks_df['industry'] != '')\n",
    "    ]\n",
    "\n",
    "    # Save new stocks data to table\n",
    "    failed_stocks = tdm.upsert_stocks(clean_stocks_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29d335e7",
   "metadata": {},
   "source": [
    "#### Get Price Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "0b9d450f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-11-29\n"
     ]
    }
   ],
   "source": [
    "# Get max trade date in ohlcv table\n",
    "max_trade_date = db.psy_query(\"select (max(trade_date)+1) as start_date from ohlcv\")['start_date'].iloc[0]\n",
    "print(max_trade_date)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "7ddb2bdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get current symbols - only select active tickers.  Additional fields for potential diagnosis.\n",
    "# indices are omitted because they don't exist within Tiingo\n",
    "symbols_df = db.psy_query(\"select ticker, company_name, asset_type from symbols where is_active = 1 and asset_type != 'index'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "38aff0e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-12-02 13:51:42,428 - INFO - Starting price data download for 10166\n",
      "2025-12-02 13:51:42,459 - INFO - Processing batch 1.0/102\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading price data...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-12-02 13:53:11,328 - ERROR - Failed to download FMTO: \"None of ['date'] are in the columns\"\n",
      "2025-12-02 13:53:41,371 - INFO - Inserted 99 rows to staging\n",
      "2025-12-02 13:53:41,372 - INFO - Processing batch 2.0/102\n",
      "2025-12-02 13:55:41,387 - INFO - Inserted 100 rows to staging\n",
      "2025-12-02 13:55:41,388 - INFO - Processing batch 3.0/102\n",
      "2025-12-02 13:57:41,385 - INFO - Inserted 100 rows to staging\n",
      "2025-12-02 13:57:41,386 - INFO - Processing batch 4.0/102\n",
      "2025-12-02 13:58:54,583 - ERROR - Failed to download LSB: \"None of ['date'] are in the columns\"\n",
      "2025-12-02 13:59:23,347 - ERROR - Failed to download AMBC: \"None of ['date'] are in the columns\"\n",
      "2025-12-02 13:59:41,420 - INFO - Inserted 98 rows to staging\n",
      "2025-12-02 13:59:41,421 - INFO - Processing batch 5.0/102\n",
      "2025-12-02 13:59:42,587 - ERROR - Failed to download NFINU: \"None of ['date'] are in the columns\"\n",
      "2025-12-02 14:00:22,222 - ERROR - Failed to download NFINW: \"None of ['date'] are in the columns\"\n",
      "2025-12-02 14:01:41,441 - INFO - Inserted 98 rows to staging\n",
      "2025-12-02 14:01:41,442 - INFO - Processing batch 6.0/102\n",
      "2025-12-02 14:03:41,469 - INFO - Inserted 100 rows to staging\n",
      "2025-12-02 14:03:41,470 - INFO - Processing batch 7.0/102\n",
      "2025-12-02 14:05:41,422 - INFO - Inserted 100 rows to staging\n",
      "2025-12-02 14:05:41,423 - INFO - Processing batch 8.0/102\n",
      "2025-12-02 14:06:38,995 - ERROR - Failed to download ATUS: \"None of ['date'] are in the columns\"\n",
      "2025-12-02 14:07:41,456 - INFO - Inserted 99 rows to staging\n",
      "2025-12-02 14:07:41,457 - INFO - Processing batch 9.0/102\n",
      "2025-12-02 14:09:41,450 - INFO - Inserted 100 rows to staging\n",
      "2025-12-02 14:09:41,452 - INFO - Processing batch 10.0/102\n",
      "2025-12-02 14:11:41,448 - INFO - Inserted 100 rows to staging\n",
      "2025-12-02 14:11:41,449 - INFO - Processing batch 11.0/102\n",
      "2025-12-02 14:13:41,456 - INFO - Inserted 100 rows to staging\n",
      "2025-12-02 14:13:41,457 - INFO - Processing batch 12.0/102\n",
      "2025-12-02 14:15:41,479 - INFO - Inserted 100 rows to staging\n",
      "2025-12-02 14:15:41,479 - INFO - Processing batch 13.0/102\n",
      "2025-12-02 14:17:41,497 - INFO - Inserted 100 rows to staging\n",
      "2025-12-02 14:17:41,498 - INFO - Processing batch 14.0/102\n",
      "2025-12-02 14:19:41,501 - INFO - Inserted 100 rows to staging\n",
      "2025-12-02 14:19:41,502 - INFO - Processing batch 15.0/102\n",
      "2025-12-02 14:21:41,517 - INFO - Inserted 100 rows to staging\n",
      "2025-12-02 14:21:41,518 - INFO - Processing batch 16.0/102\n",
      "2025-12-02 14:23:41,543 - INFO - Inserted 100 rows to staging\n",
      "2025-12-02 14:23:41,544 - INFO - Processing batch 17.0/102\n",
      "2025-12-02 14:25:41,546 - INFO - Inserted 100 rows to staging\n",
      "2025-12-02 14:25:41,547 - INFO - Processing batch 18.0/102\n",
      "2025-12-02 14:26:10,330 - ERROR - Failed to download CHEK: \"None of ['date'] are in the columns\"\n",
      "2025-12-02 14:27:41,551 - INFO - Inserted 99 rows to staging\n",
      "2025-12-02 14:27:41,552 - INFO - Processing batch 19.0/102\n",
      "2025-12-02 14:28:11,536 - ERROR - Failed to download CLSD: \"None of ['date'] are in the columns\"\n",
      "2025-12-02 14:29:41,575 - INFO - Inserted 99 rows to staging\n",
      "2025-12-02 14:29:41,576 - INFO - Processing batch 20.0/102\n",
      "2025-12-02 14:31:41,621 - INFO - Inserted 100 rows to staging\n",
      "2025-12-02 14:31:41,622 - INFO - Processing batch 21.0/102\n",
      "2025-12-02 14:33:41,605 - INFO - Inserted 100 rows to staging\n",
      "2025-12-02 14:33:41,607 - INFO - Processing batch 22.0/102\n",
      "2025-12-02 14:35:41,644 - INFO - Inserted 100 rows to staging\n",
      "2025-12-02 14:35:41,644 - INFO - Processing batch 23.0/102\n",
      "2025-12-02 14:37:41,654 - INFO - Inserted 100 rows to staging\n",
      "2025-12-02 14:37:41,655 - INFO - Processing batch 24.0/102\n",
      "2025-12-02 14:39:41,665 - INFO - Inserted 100 rows to staging\n",
      "2025-12-02 14:39:41,666 - INFO - Processing batch 25.0/102\n",
      "2025-12-02 14:41:41,684 - INFO - Inserted 100 rows to staging\n",
      "2025-12-02 14:41:41,684 - INFO - Processing batch 26.0/102\n",
      "2025-12-02 14:42:04,444 - ERROR - Failed to download DRH: \"None of ['date'] are in the columns\"\n",
      "2025-12-02 14:43:41,692 - INFO - Inserted 99 rows to staging\n",
      "2025-12-02 14:43:41,693 - INFO - Processing batch 27.0/102\n",
      "2025-12-02 14:45:41,720 - INFO - Inserted 100 rows to staging\n",
      "2025-12-02 14:45:41,721 - INFO - Processing batch 28.0/102\n",
      "2025-12-02 14:47:41,763 - INFO - Inserted 100 rows to staging\n",
      "2025-12-02 14:47:41,764 - INFO - Processing batch 29.0/102\n",
      "2025-12-02 14:49:41,780 - INFO - Inserted 100 rows to staging\n",
      "2025-12-02 14:49:41,780 - INFO - Processing batch 30.0/102\n",
      "2025-12-02 14:51:41,792 - INFO - Inserted 100 rows to staging\n",
      "2025-12-02 14:51:41,793 - INFO - Processing batch 31.0/102\n",
      "2025-12-02 14:53:41,813 - INFO - Inserted 100 rows to staging\n",
      "2025-12-02 14:53:41,814 - INFO - Processing batch 32.0/102\n",
      "2025-12-02 14:55:22,607 - ERROR - Failed to download FDHT: \"None of ['date'] are in the columns\"\n",
      "2025-12-02 14:55:41,795 - INFO - Inserted 99 rows to staging\n",
      "2025-12-02 14:55:41,796 - INFO - Processing batch 33.0/102\n",
      "2025-12-02 14:55:53,886 - ERROR - Failed to download FDWM: \"None of ['date'] are in the columns\"\n",
      "2025-12-02 14:57:41,819 - INFO - Inserted 99 rows to staging\n",
      "2025-12-02 14:57:41,820 - INFO - Processing batch 34.0/102\n",
      "2025-12-02 14:59:41,835 - INFO - Inserted 100 rows to staging\n",
      "2025-12-02 14:59:41,836 - INFO - Processing batch 35.0/102\n",
      "2025-12-02 15:01:41,848 - INFO - Inserted 100 rows to staging\n",
      "2025-12-02 15:01:41,849 - INFO - Processing batch 36.0/102\n",
      "2025-12-02 15:01:51,434 - ERROR - Failed to download FOSLL: \"None of ['date'] are in the columns\"\n",
      "2025-12-02 15:02:53,849 - ERROR - Failed to download FSBD: \"None of ['date'] are in the columns\"\n",
      "2025-12-02 15:03:09,459 - ERROR - Failed to download FSLD: \"None of ['date'] are in the columns\"\n",
      "2025-12-02 15:03:41,904 - INFO - Inserted 97 rows to staging\n",
      "2025-12-02 15:03:41,905 - INFO - Processing batch 37.0/102\n",
      "2025-12-02 15:05:41,922 - INFO - Inserted 100 rows to staging\n",
      "2025-12-02 15:05:41,923 - INFO - Processing batch 38.0/102\n",
      "2025-12-02 15:05:52,702 - ERROR - Failed to download FSST: \"None of ['date'] are in the columns\"\n",
      "2025-12-02 15:07:41,945 - INFO - Inserted 99 rows to staging\n",
      "2025-12-02 15:07:41,946 - INFO - Processing batch 39.0/102\n",
      "2025-12-02 15:09:41,958 - INFO - Inserted 100 rows to staging\n",
      "2025-12-02 15:09:41,959 - INFO - Processing batch 40.0/102\n",
      "2025-12-02 15:10:15,544 - ERROR - Failed to download GNFT: \"None of ['date'] are in the columns\"\n",
      "2025-12-02 15:11:42,001 - INFO - Inserted 99 rows to staging\n",
      "2025-12-02 15:11:42,002 - INFO - Processing batch 41.0/102\n",
      "2025-12-02 15:13:41,993 - INFO - Inserted 100 rows to staging\n",
      "2025-12-02 15:13:41,994 - INFO - Processing batch 42.0/102\n",
      "2025-12-02 15:15:42,008 - INFO - Inserted 100 rows to staging\n",
      "2025-12-02 15:15:42,009 - INFO - Processing batch 43.0/102\n",
      "2025-12-02 15:17:15,723 - ERROR - Failed to download HONE: \"None of ['date'] are in the columns\"\n",
      "2025-12-02 15:17:42,054 - INFO - Inserted 99 rows to staging\n",
      "2025-12-02 15:17:42,055 - INFO - Processing batch 44.0/102\n",
      "2025-12-02 15:19:42,050 - INFO - Inserted 100 rows to staging\n",
      "2025-12-02 15:19:42,051 - INFO - Processing batch 45.0/102\n",
      "2025-12-02 15:21:42,064 - INFO - Inserted 100 rows to staging\n",
      "2025-12-02 15:21:42,064 - INFO - Processing batch 46.0/102\n",
      "2025-12-02 15:23:42,102 - INFO - Inserted 100 rows to staging\n",
      "2025-12-02 15:23:42,103 - INFO - Processing batch 47.0/102\n",
      "2025-12-02 15:25:19,320 - ERROR - Failed to download INFA: \"None of ['date'] are in the columns\"\n",
      "2025-12-02 15:25:42,122 - INFO - Inserted 99 rows to staging\n",
      "2025-12-02 15:25:42,123 - INFO - Processing batch 48.0/102\n",
      "2025-12-02 15:26:48,216 - ERROR - Failed to download IPG: \"None of ['date'] are in the columns\"\n",
      "2025-12-02 15:27:42,127 - INFO - Inserted 99 rows to staging\n",
      "2025-12-02 15:27:42,128 - INFO - Processing batch 49.0/102\n",
      "2025-12-02 15:29:42,171 - INFO - Inserted 100 rows to staging\n",
      "2025-12-02 15:29:42,171 - INFO - Processing batch 50.0/102\n",
      "2025-12-02 15:31:42,179 - INFO - Inserted 100 rows to staging\n",
      "2025-12-02 15:31:42,180 - INFO - Processing batch 51.0/102\n",
      "2025-12-02 15:33:42,217 - INFO - Inserted 100 rows to staging\n",
      "2025-12-02 15:33:42,217 - INFO - Processing batch 52.0/102\n",
      "2025-12-02 15:35:42,252 - INFO - Inserted 100 rows to staging\n",
      "2025-12-02 15:35:42,253 - INFO - Processing batch 53.0/102\n",
      "2025-12-02 15:37:42,279 - INFO - Inserted 100 rows to staging\n",
      "2025-12-02 15:37:42,280 - INFO - Processing batch 54.0/102\n",
      "2025-12-02 15:39:11,186 - ERROR - Failed to download LCG: \"None of ['date'] are in the columns\"\n",
      "2025-12-02 15:39:42,292 - INFO - Inserted 99 rows to staging\n",
      "2025-12-02 15:39:42,293 - INFO - Processing batch 55.0/102\n",
      "2025-12-02 15:41:42,319 - INFO - Inserted 100 rows to staging\n",
      "2025-12-02 15:41:42,320 - INFO - Processing batch 56.0/102\n",
      "2025-12-02 15:43:42,343 - INFO - Inserted 100 rows to staging\n",
      "2025-12-02 15:43:42,344 - INFO - Processing batch 57.0/102\n",
      "2025-12-02 15:45:42,303 - INFO - Inserted 100 rows to staging\n",
      "2025-12-02 15:45:42,304 - INFO - Processing batch 58.0/102\n",
      "2025-12-02 15:47:42,355 - INFO - Inserted 100 rows to staging\n",
      "2025-12-02 15:47:42,355 - INFO - Processing batch 59.0/102\n",
      "2025-12-02 15:47:47,106 - ERROR - Failed to download MESA: \"None of ['date'] are in the columns\"\n",
      "2025-12-02 15:49:42,357 - INFO - Inserted 99 rows to staging\n",
      "2025-12-02 15:49:42,357 - INFO - Processing batch 60.0/102\n",
      "2025-12-02 15:50:07,532 - ERROR - Failed to download MTSR: \"None of ['date'] are in the columns\"\n",
      "2025-12-02 15:51:42,380 - INFO - Inserted 99 rows to staging\n",
      "2025-12-02 15:51:42,381 - INFO - Processing batch 61.0/102\n",
      "2025-12-02 15:52:06,402 - ERROR - Failed to download MRC: \"None of ['date'] are in the columns\"\n",
      "2025-12-02 15:53:42,401 - INFO - Inserted 99 rows to staging\n",
      "2025-12-02 15:53:42,402 - INFO - Processing batch 62.0/102\n",
      "2025-12-02 15:55:42,401 - INFO - Inserted 100 rows to staging\n",
      "2025-12-02 15:55:42,402 - INFO - Processing batch 63.0/102\n",
      "2025-12-02 15:57:42,440 - INFO - Inserted 100 rows to staging\n",
      "2025-12-02 15:57:42,441 - INFO - Processing batch 64.0/102\n",
      "2025-12-02 15:59:42,453 - INFO - Inserted 100 rows to staging\n",
      "2025-12-02 15:59:42,454 - INFO - Processing batch 65.0/102\n",
      "2025-12-02 16:01:42,455 - INFO - Inserted 100 rows to staging\n",
      "2025-12-02 16:01:42,456 - INFO - Processing batch 66.0/102\n",
      "2025-12-02 16:03:42,502 - INFO - Inserted 100 rows to staging\n",
      "2025-12-02 16:03:42,503 - INFO - Processing batch 67.0/102\n",
      "2025-12-02 16:05:42,506 - INFO - Inserted 100 rows to staging\n",
      "2025-12-02 16:05:42,507 - INFO - Processing batch 68.0/102\n",
      "2025-12-02 16:06:05,362 - ERROR - Failed to download OPT: \"None of ['date'] are in the columns\"\n",
      "2025-12-02 16:07:42,540 - INFO - Inserted 99 rows to staging\n",
      "2025-12-02 16:07:42,541 - INFO - Processing batch 69.0/102\n",
      "2025-12-02 16:09:42,574 - INFO - Inserted 100 rows to staging\n",
      "2025-12-02 16:09:42,575 - INFO - Processing batch 70.0/102\n",
      "2025-12-02 16:11:42,551 - INFO - Inserted 100 rows to staging\n",
      "2025-12-02 16:11:42,552 - INFO - Processing batch 71.0/102\n",
      "2025-12-02 16:12:53,356 - ERROR - Failed to download PINC: \"None of ['date'] are in the columns\"\n",
      "2025-12-02 16:13:42,604 - INFO - Inserted 99 rows to staging\n",
      "2025-12-02 16:13:42,605 - INFO - Processing batch 72.0/102\n",
      "2025-12-02 16:15:42,718 - INFO - Inserted 100 rows to staging\n",
      "2025-12-02 16:15:42,720 - INFO - Processing batch 73.0/102\n",
      "2025-12-02 16:17:42,587 - INFO - Inserted 100 rows to staging\n",
      "2025-12-02 16:17:42,588 - INFO - Processing batch 74.0/102\n",
      "2025-12-02 16:18:15,083 - ERROR - Failed to download PTNT: \"None of ['date'] are in the columns\"\n",
      "2025-12-02 16:18:30,633 - ERROR - Failed to download PVBC: \"None of ['date'] are in the columns\"\n",
      "2025-12-02 16:19:42,642 - INFO - Inserted 98 rows to staging\n",
      "2025-12-02 16:19:42,643 - INFO - Processing batch 75.0/102\n",
      "2025-12-02 16:21:42,651 - INFO - Inserted 100 rows to staging\n",
      "2025-12-02 16:21:42,652 - INFO - Processing batch 76.0/102\n",
      "2025-12-02 16:22:31,864 - ERROR - Failed to download RAYC: \"None of ['date'] are in the columns\"\n",
      "2025-12-02 16:23:42,654 - INFO - Inserted 99 rows to staging\n",
      "2025-12-02 16:23:42,655 - INFO - Processing batch 77.0/102\n",
      "2025-12-02 16:25:42,669 - INFO - Inserted 100 rows to staging\n",
      "2025-12-02 16:25:42,670 - INFO - Processing batch 78.0/102\n",
      "2025-12-02 16:27:42,689 - INFO - Inserted 100 rows to staging\n",
      "2025-12-02 16:27:42,690 - INFO - Processing batch 79.0/102\n",
      "2025-12-02 16:29:42,715 - INFO - Inserted 100 rows to staging\n",
      "2025-12-02 16:29:42,716 - INFO - Processing batch 80.0/102\n",
      "2025-12-02 16:31:42,727 - INFO - Inserted 100 rows to staging\n",
      "2025-12-02 16:31:42,727 - INFO - Processing batch 81.0/102\n",
      "2025-12-02 16:33:42,749 - INFO - Inserted 100 rows to staging\n",
      "2025-12-02 16:33:42,750 - INFO - Processing batch 82.0/102\n",
      "2025-12-02 16:35:42,752 - INFO - Inserted 100 rows to staging\n",
      "2025-12-02 16:35:42,753 - INFO - Processing batch 83.0/102\n",
      "2025-12-02 16:37:42,799 - INFO - Inserted 100 rows to staging\n",
      "2025-12-02 16:37:42,800 - INFO - Processing batch 84.0/102\n",
      "2025-12-02 16:38:48,767 - ERROR - Failed to download SOND: \"None of ['date'] are in the columns\"\n",
      "2025-12-02 16:39:42,822 - INFO - Inserted 99 rows to staging\n",
      "2025-12-02 16:39:42,823 - INFO - Processing batch 85.0/102\n",
      "2025-12-02 16:41:35,662 - ERROR - Failed to download SRDX: \"None of ['date'] are in the columns\"\n",
      "2025-12-02 16:41:42,798 - INFO - Inserted 99 rows to staging\n",
      "2025-12-02 16:41:42,799 - INFO - Processing batch 86.0/102\n",
      "2025-12-02 16:43:42,858 - INFO - Inserted 100 rows to staging\n",
      "2025-12-02 16:43:42,859 - INFO - Processing batch 87.0/102\n",
      "2025-12-02 16:45:42,855 - INFO - Inserted 100 rows to staging\n",
      "2025-12-02 16:45:42,856 - INFO - Processing batch 88.0/102\n",
      "2025-12-02 16:47:27,226 - ERROR - Failed to download TEAF: \"None of ['date'] are in the columns\"\n",
      "2025-12-02 16:47:42,855 - INFO - Inserted 99 rows to staging\n",
      "2025-12-02 16:47:42,856 - INFO - Processing batch 89.0/102\n",
      "2025-12-02 16:49:42,884 - INFO - Inserted 100 rows to staging\n",
      "2025-12-02 16:49:42,884 - INFO - Processing batch 90.0/102\n",
      "2025-12-02 16:51:42,936 - INFO - Inserted 100 rows to staging\n",
      "2025-12-02 16:51:42,937 - INFO - Processing batch 91.0/102\n",
      "2025-12-02 16:53:42,906 - INFO - Inserted 100 rows to staging\n",
      "2025-12-02 16:53:42,907 - INFO - Processing batch 92.0/102\n",
      "2025-12-02 16:55:42,940 - INFO - Inserted 100 rows to staging\n",
      "2025-12-02 16:55:42,941 - INFO - Processing batch 93.0/102\n",
      "2025-12-02 16:57:42,937 - INFO - Inserted 100 rows to staging\n",
      "2025-12-02 16:57:42,938 - INFO - Processing batch 94.0/102\n",
      "2025-12-02 16:59:42,972 - INFO - Inserted 100 rows to staging\n",
      "2025-12-02 16:59:42,974 - INFO - Processing batch 95.0/102\n",
      "2025-12-02 17:01:42,966 - INFO - Inserted 100 rows to staging\n",
      "2025-12-02 17:01:42,968 - INFO - Processing batch 96.0/102\n",
      "2025-12-02 17:02:20,135 - ERROR - Failed to download VMEO: \"None of ['date'] are in the columns\"\n",
      "2025-12-02 17:03:21,344 - ERROR - Failed to download VRNT: \"None of ['date'] are in the columns\"\n",
      "2025-12-02 17:03:42,971 - INFO - Inserted 98 rows to staging\n",
      "2025-12-02 17:03:42,971 - INFO - Processing batch 97.0/102\n",
      "2025-12-02 17:05:42,973 - INFO - Inserted 100 rows to staging\n",
      "2025-12-02 17:05:42,974 - INFO - Processing batch 98.0/102\n",
      "2025-12-02 17:07:42,983 - INFO - Inserted 100 rows to staging\n",
      "2025-12-02 17:07:42,984 - INFO - Processing batch 99.0/102\n",
      "2025-12-02 17:07:49,012 - ERROR - Failed to download WSBCP: \"None of ['date'] are in the columns\"\n",
      "2025-12-02 17:09:42,993 - INFO - Inserted 99 rows to staging\n",
      "2025-12-02 17:09:42,994 - INFO - Processing batch 100.0/102\n",
      "2025-12-02 17:10:37,493 - ERROR - Failed to download SYNB: \"None of ['date'] are in the columns\"\n",
      "2025-12-02 17:11:42,988 - INFO - Inserted 99 rows to staging\n",
      "2025-12-02 17:11:42,989 - INFO - Processing batch 101.0/102\n",
      "2025-12-02 17:13:43,038 - INFO - Inserted 100 rows to staging\n",
      "2025-12-02 17:13:43,039 - INFO - Processing batch 102.0/102\n",
      "2025-12-02 17:14:57,405 - ERROR - Failed to download ZXZZT: \"None of ['date'] are in the columns\"\n",
      "2025-12-02 17:15:02,224 - INFO - Inserted 65 rows to staging\n",
      "2025-12-02 17:15:02,225 - INFO - Download complete.  Failed tickers: 36\n"
     ]
    }
   ],
   "source": [
    "# Get pricing data - This is a 200 minute operation for a day's prices\n",
    "print(\"Downloading price data...\")\n",
    "ticker_list = symbols_df['ticker'].tolist() \n",
    "failed_prices = tdm.download_price_data(ticker_list, max_trade_date)\n",
    "# failed_prices = tdm.download_price_data(ticker_list, '2025-11-18', '2025-11-21')  # was used to backfill a hole in the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "6e297ff6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-12-02 17:28:39,328 - INFO - Validating staging data\n",
      "2025-12-02 17:28:39,372 - INFO - Staging table contains 10,130 rows\n",
      "2025-12-02 17:28:39,372 - INFO - Validation passed - no issues found\n",
      "2025-12-02 17:28:39,373 - INFO - Moving staging data to production...\n",
      "2025-12-02 17:28:39,449 - INFO - Moved 10,130 rows to production\n",
      "2025-12-02 17:28:39,468 - INFO - Successfully moved staging to production\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validating and moving to production...\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Validate and move price data to ohlcv table\n",
    "print(\"Validating and moving to production...\")\n",
    "tdm.validate_and_move_staging()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "84e5be9d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checking for stale tickers...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-12-02 17:28:44,472 - INFO - Found 0 stale tickers (no prices in 30 days)\n",
      "2025-12-02 17:28:44,472 - INFO - No stale tickers to mark inactive\n"
     ]
    }
   ],
   "source": [
    "# D --- Mark stale tickers as inactive\n",
    "print(\"Checking for stale tickers...\")\n",
    "\n",
    "# First count how many will be affected\n",
    "count_query = \"\"\"\n",
    "    SELECT COUNT(*) as cnt\n",
    "    FROM symbols s\n",
    "    WHERE s.is_active = 1\n",
    "      AND s.asset_type != 'index'\n",
    "      AND s.date_loaded < CURRENT_DATE - INTERVAL '30 days'\n",
    "      AND s.ticker NOT IN (\n",
    "          SELECT DISTINCT ticker \n",
    "          FROM ohlcv \n",
    "          WHERE trade_date >= CURRENT_DATE - INTERVAL '30 days'\n",
    "      )\n",
    "\"\"\"\n",
    "stale_count = db.psy_query(count_query)['cnt'].iloc[0]\n",
    "logger.info(f\"Found {stale_count} stale tickers (no prices in 30 days)\")\n",
    "\n",
    "if stale_count > 0:\n",
    "    # Now run the update\n",
    "    update_sql = \"\"\"\n",
    "        UPDATE symbols s\n",
    "        SET \n",
    "            is_active = 0,\n",
    "            end_date = COALESCE(\n",
    "                (SELECT MAX(trade_date) FROM ohlcv WHERE ticker = s.ticker),\n",
    "                CURRENT_DATE\n",
    "            )\n",
    "        WHERE s.is_active = 1\n",
    "          AND s.asset_type != 'index'\n",
    "          AND s.date_loaded < CURRENT_DATE - INTERVAL '30 days'\n",
    "          AND s.ticker NOT IN (\n",
    "              SELECT DISTINCT ticker \n",
    "              FROM ohlcv \n",
    "              WHERE trade_date >= CURRENT_DATE - INTERVAL '30 days'\n",
    "          )\n",
    "    \"\"\"\n",
    "    db.execute_sql(update_sql)\n",
    "    logger.info(f\"Marked {stale_count} tickers as inactive\")\n",
    "else:\n",
    "    logger.info(\"No stale tickers to mark inactive\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48937f87",
   "metadata": {},
   "source": [
    "#### Patch Up Not Associated With Process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f4d33ce1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Backfilling prices for 118 ETFs...\n"
     ]
    }
   ],
   "source": [
    "# Get the mislabeled ETFs added on 11/21\n",
    "new_etfs = db.psy_query(\"\"\"\n",
    "    SELECT ticker FROM symbols \n",
    "    WHERE date_loaded = '2025-11-21' \n",
    "    AND asset_type = 'etf'\n",
    "\"\"\")\n",
    "\n",
    "print(f\"Backfilling prices for {len(new_etfs)} ETFs...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8bf0fca3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-11-23 17:39:31,398 - INFO - Starting price data download for 118\n",
      "2025-11-23 17:39:31,429 - INFO - Processing batch 1.0/2\n",
      "2025-11-23 17:40:23,098 - ERROR - Failed to download MELT: \"None of ['date'] are in the columns\"\n",
      "2025-11-23 17:40:41,102 - ERROR - Failed to download BA-P: \"None of ['date'] are in the columns\"\n",
      "2025-11-23 17:40:54,302 - ERROR - Failed to download PA-P: \"None of ['date'] are in the columns\"\n",
      "2025-11-23 17:41:31,154 - INFO - Inserted 50593 rows to staging\n",
      "2025-11-23 17:41:31,154 - INFO - Processing batch 2.0/2\n",
      "2025-11-23 17:41:37,516 - ERROR - Failed to download UA-P: \"None of ['date'] are in the columns\"\n",
      "2025-11-23 17:41:52,063 - INFO - Inserted 7784 rows to staging\n",
      "2025-11-23 17:41:52,064 - INFO - Download complete.  Failed tickers: 4\n"
     ]
    }
   ],
   "source": [
    "# Fetch 10-year history\n",
    "failed = tdm.download_price_data(new_etfs['ticker'].tolist(), start_date='2015-01-01')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1a59dcbd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-11-23 17:41:58,035 - INFO - Validating staging data\n",
      "2025-11-23 17:41:58,123 - INFO - Staging table contains 58,377 rows\n",
      "2025-11-23 17:41:58,124 - INFO - Validation passed - no issues found\n",
      "2025-11-23 17:41:58,125 - INFO - Moving staging data to production...\n",
      "2025-11-23 17:42:00,897 - INFO - Moved 58,377 rows to production\n",
      "2025-11-23 17:42:00,916 - INFO - Successfully moved staging to production\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Validate and move to production\n",
    "tdm.validate_and_move_staging()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "530f1172",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Backfilling prices for 8 ETFs...\n"
     ]
    }
   ],
   "source": [
    "# Get the missing indices added on 11/23\n",
    "index_df = db.psy_query(\"\"\"\n",
    "    SELECT ticker FROM symbols \n",
    "    WHERE date_loaded = '2025-11-23' \n",
    "    AND asset_type = 'index'\n",
    "\"\"\")\n",
    "\n",
    "print(f\"Backfilling prices for {len(index_df)} ETFs...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cfd6855",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-11-23 18:15:56,249 - INFO - Starting price data download for 8\n",
      "2025-11-23 18:15:56,269 - INFO - Processing batch 1.0/1\n",
      "2025-11-23 18:15:56,406 - ERROR - b'{\"detail\":\"Error: Ticker \\'GSPC\\' not found\"}'\n",
      "2025-11-23 18:15:56,407 - ERROR - Failed to download GSPC: 404 Client Error: Not Found for url: https://api.tiingo.com/tiingo/daily/GSPC/prices?format=json&resampleFreq=daily&startDate=2015-01-01\n",
      "2025-11-23 18:15:57,582 - ERROR - b'{\"detail\":\"Error: Ticker \\'DJI\\' not found\"}'\n",
      "2025-11-23 18:15:57,582 - ERROR - Failed to download DJI: 404 Client Error: Not Found for url: https://api.tiingo.com/tiingo/daily/DJI/prices?format=json&resampleFreq=daily&startDate=2015-01-01\n",
      "2025-11-23 18:15:58,764 - ERROR - b'{\"detail\":\"Error: Ticker \\'NDX\\' not found\"}'\n",
      "2025-11-23 18:15:58,768 - ERROR - Failed to download NDX: 404 Client Error: Not Found for url: https://api.tiingo.com/tiingo/daily/NDX/prices?format=json&resampleFreq=daily&startDate=2015-01-01\n",
      "2025-11-23 18:15:59,970 - ERROR - b'{\"detail\":\"Error: Ticker \\'RUT\\' not found\"}'\n",
      "2025-11-23 18:15:59,971 - ERROR - Failed to download RUT: 404 Client Error: Not Found for url: https://api.tiingo.com/tiingo/daily/RUT/prices?format=json&resampleFreq=daily&startDate=2015-01-01\n",
      "2025-11-23 18:16:01,174 - ERROR - b'{\"detail\":\"Error: Ticker \\'VIX\\' not found\"}'\n",
      "2025-11-23 18:16:01,175 - ERROR - Failed to download VIX: 404 Client Error: Not Found for url: https://api.tiingo.com/tiingo/daily/VIX/prices?format=json&resampleFreq=daily&startDate=2015-01-01\n",
      "2025-11-23 18:16:02,385 - ERROR - b'{\"detail\":\"Error: Ticker \\'VXN\\' not found\"}'\n",
      "2025-11-23 18:16:02,385 - ERROR - Failed to download VXN: 404 Client Error: Not Found for url: https://api.tiingo.com/tiingo/daily/VXN/prices?format=json&resampleFreq=daily&startDate=2015-01-01\n",
      "2025-11-23 18:16:03,569 - ERROR - b'{\"detail\":\"Error: Ticker \\'RVX\\' not found\"}'\n",
      "2025-11-23 18:16:03,570 - ERROR - Failed to download RVX: 404 Client Error: Not Found for url: https://api.tiingo.com/tiingo/daily/RVX/prices?format=json&resampleFreq=daily&startDate=2015-01-01\n",
      "2025-11-23 18:16:04,784 - ERROR - b'{\"detail\":\"Error: Ticker \\'DXY\\' not found\"}'\n",
      "2025-11-23 18:16:04,785 - ERROR - Failed to download DXY: 404 Client Error: Not Found for url: https://api.tiingo.com/tiingo/daily/DXY/prices?format=json&resampleFreq=daily&startDate=2015-01-01\n",
      "2025-11-23 18:16:04,786 - INFO - Download complete.  Failed tickers: 8\n"
     ]
    }
   ],
   "source": [
    "# Fetch 10-year history - Ruh row.  Proprietary but we have the ETF equivalents.  A table index_mapping was created\n",
    "failed = tdm.download_price_data(index_df['ticker'].tolist(), start_date='2015-01-01')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "quant_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
